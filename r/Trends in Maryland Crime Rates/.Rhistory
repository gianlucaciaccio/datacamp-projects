# Check the name of the current folder
current_dir <- getwd()
print(current_dir)
# List all files in this folder
file_list <- list.files()
print(file_list)
# List files inside the datasets folder
file_list_ds <- list.files(path = "datasets/")
print(file_list_ds)
# View the first 20 lines of road-accidents.csv in the datasets folder
accidents_head <- readLines(con = "datasets/road-accidents.csv", n = 20)
print(accidents_head)
# Load the tidyverse library
library(tidyverse)
# Read in road-accidents.csv and set the comment argument
car_acc <- read_delim(file = "datasets/road-accidents.csv", delim = "|", comment = "#")
# Save the number of rows columns
rows_and_cols <- dim(car_acc)
print(rows_and_cols)
# Generate an overview of the data frame
str(car_acc)
# Display the last six rows of the data frame.
tail(car_acc)
# Compute summary statistics of all columns in the car_acc data frame
dat_summ <- summary(car_acc)
print(dat_summ)
# Deselect the state column and create a pairwise scatterplot
library(GGally)
car_acc %>%
select(-state) %>%
ggpairs()
# Using pipes, remove the state column and then compute the correlation coefficient for all column pairs
corr_col <- car_acc %>% select(-state) %>% cor()
# Print the correlation coefficient for all column pairs
print(corr_col)
# Use lm to fit a multivariate linear regression model
fit_reg <- lm(drvr_fatl_col_bmiles ~ perc_fatl_speed + perc_fatl_alcohol + perc_fatl_1st_time, data = car_acc)
# Retrieve the regression coefficients from the model fit
fit_coef <- coef(fit_reg)
print(fit_coef)
# Center and standardise the three feature columns
car_acc_standised <- car_acc %>%
mutate(perc_fatl_speed = scale(perc_fatl_speed),
perc_fatl_alcohol = scale(perc_fatl_alcohol),
perc_fatl_1st_time = scale(perc_fatl_1st_time) )
# Perform PCA on standardized features
pca_fit <- princomp(car_acc_standised[, c("perc_fatl_speed", "perc_fatl_alcohol", "perc_fatl_1st_time")])
# Obtain the proportion of variance explained by each principle component
pr_var <- pca_fit$sdev^2
pve <- pr_var / sum(pr_var)
# Plot the proportion of variance explained, draw a point plot connected with lines
data_frame( comp_id=1:length(pve) , pve ) %>%
ggplot( aes(x=comp_id , y=pve) ) + geom_point() + geom_line() +
coord_cartesian(ylim=c(0,1)) +
labs(x="Component",
y="Explained variance")
# Compute the cumulative proportion of variance and extract the variance explained by the first two principal components
cve <- cumsum(pve)
cve_pc2 <- cve[2]
print(cve_pc2)
# Get the principle component scores from the PCA fit
pcomp1 <- pca_fit$scores[,1]
pcomp2 <- pca_fit$scores[,2]
# Plot the first 2 principle components in a scatterplot using ggplot
data_frame(pcomp1,pcomp2) %>%
ggplot(aes(x = pcomp1, y = pcomp2)) +
geom_point()
# Create a vector of 1 to 10
k_vec <- 1:10
# Initialise vector of inertias
inertias <- rep(NA, length(k_vec))
# Initialise empty list to save K-mean fits
mykm <- list()
# Set the seed of random number generator
set.seed(1)
for (k in k_vec) {
# for each k, fit a K-mean model with k clusters and save it in the mykm list
mykm[[k]] <- kmeans(car_acc_standised[,c(3,4,5)], k, nstart=50)
# for each k, get the within-cluster sum-of-squares and save
inertias[k] <- mykm[[k]]$tot.withinss
}
# Plot the within-cluster sum-of-squares against the number of clusters used
data_frame(k_vec,inertias) %>%
ggplot( aes(x = k_vec, y = inertias) ) +
geom_point() + geom_line() +
labs(x="Number of clusters", y="Intertias")
# Obtain cluster-ids from the kmeans fit with k=3
cluster_id <- as.factor(mykm[[3]]$cluster)
# Color the points of the principle component plot according to their cluster number
data_frame(pcomp1,pcomp2) %>%
ggplot(aes(x=pcomp1,y=pcomp2)) + geom_point(aes(col = cluster_id)) +
labs(x="Principle Component 1",
y="Principle Component 2")
# add the cluster_id into the original data frame
car_acc$cluster <- cluster_id
# use the pipe operator on the car_acc data frame to first remove the drvr_fatl_col_bmiles column,
# then convert the data frame into a long format and then create a violin plot using ggplot
car_acc %>%  select(-drvr_fatl_col_bmiles) %>%
gather(key=feature,value=percent,-state,-cluster) %>%
ggplot( aes(x=feature,y=percent , fill=cluster) ) +
geom_violin( ) +
coord_flip()
# Read in the miles-driven.csv file
miles_driven <- read_delim( file="datasets/miles-driven.csv", delim = '|' )
# Join miles_driven with car_acc and add num_drvr_fatl_col
carr_acc_joined <- car_acc  %>%
left_join(miles_driven, by = "state")  %>%
mutate(num_drvr_fatl_col = drvr_fatl_col_bmiles * million_miles_annually / 1000)
# Group the new data frame, select relevant variables, and summarise
carr_acc_joined_summ <- carr_acc_joined %>%
group_by(cluster) %>%
select(cluster,num_drvr_fatl_col) %>%
summarise(count=n(),
mean=mean(num_drvr_fatl_col),
sum=sum(num_drvr_fatl_col))
print(carr_acc_joined_summ)
# Compare the total fatal accident sum across clusters using a bar plot
carr_acc_joined_summ %>%
ggplot(aes(x=cluster, y=sum)) +
geom_bar(aes(fill = cluster), stat = "identity", show.legend = F)
# Which cluster would you choose?
cluster_num <- sample(1:3,size=1)
print(cluster_num)
rm(list = ls())
setwd("~/datascience-projects/DataCamp/projects/r/Trends in Maryland Crime Rates")
# Load the packages
library(tidyverse)
library(lubridate)
# Read in the crime data
crime_raw <- read_csv("datasets/Violent_Crime_by_County_1975_to_2016.csv")
# Select and mutate columns the needed columns
crime_use <- crime_raw %>%
select(JURISDICTION, YEAR, POPULATION, crime_rate = `VIOLENT CRIME RATE PER 100,000 PEOPLE`) %>%
mutate(YEAR_2 = year(mdy_hms(YEAR)))
# Peek at the data
head(crime_use)
# Plot the data as lines and linear trend lines
ggplot(crime_use, aes(x = YEAR_2, y = crime_rate, group = JURISDICTION)) +
geom_line() +
stat_smooth(method = "lm", se = FALSE, size = 0.5)
# Mutate data to create another year column, YEAR_3
crime_use <-
crime_use %>%
mutate(YEAR_3 = YEAR_2 - min(YEAR_2))
# load the lmerTest package
library(lmerTest)
# load the lmerTest package
library(lmerTest)
# Build a lmer and save it as lmer_crime
lmer_crime <- lmer(crime_rate ~ YEAR_3 + (YEAR_3|JURISDICTION), crime_use)
# Print the model output
lmer_crime
# Examine the model outputs using summary
summary(lmer_crime)
# This is for readability
noquote("**** Fixed-effects ****")
# Use fixef() to view fixed-effects
fixef(lmer_crime)
# This is for readability
noquote("**** Random-effects ****")
# Use ranef() to view random-effects
ranef(lmer_crime)
# Add the fixed-effect to the random-effect and save as county_slopes
county_slopes <- fixef(lmer_crime)["YEAR_3"] + ranef(lmer_crime)$JURISDICTION["YEAR_3"]
# Add a new column with county names
(county_slopes <-
county_slopes %>%
rownames_to_column("county"))
# Load usmap package
library(usmap)
# load and filter map data
county_map <- us_map(regions = "counties", include = "MD")
# See which counties are not in both datasets
county_slopes %>% anti_join(county_map, by = "county")
county_map %>% anti_join(county_slopes, by = "county")
# Rename crime_names county
county_slopes  <- county_slopes  %>%
mutate(county = ifelse(county == "Baltimore City", "Baltimore city", county))
# Merge the map and slope data frames
both_data <-
full_join(county_map, county_slopes, by = "county")
# Peek at the data
head(both_data)
# Set the notebook's plot settings
options(repr.plot.width=10, repr.plot.height=5)
# Plot the results
crime_map <-
ggplot(data = both_data, aes(x = long, y = lat, group = county, fill = YEAR_3)) +
geom_polygon() +
scale_fill_continuous(name = expression(atop("Change in crime rate","(Number year"^-1*")")),
low = "skyblue", high = "gold")
# Look at the map
crime_map
both_data
county_map
county_slopes
View(crime_raw)
# load and filter map data
county_map <- us_map(regions = "counties", include = "MD")
View(county_map)
?us_map
# Look at the map
crime_map
# Plot the results
crime_map <-
ggplot(data = both_data, aes(x = x, y = y, group = county, fill = YEAR_3)) +
geom_polygon() +
scale_fill_continuous(name = expression(atop("Change in crime rate","(Number year"^-1*")")),
low = "skyblue", high = "gold")
# Look at the map
crime_map
county_map <- rename(x=long,y=lat)
county_map <- rename(x=long,y=lat)
?rename
county_map <- county_map %>% rename(x=long,y=lat)
county_map <- county_map %>% rename(long = x, lat = y)
# load and filter map data
county_map <- us_map(regions = "counties", include = "MD")
county_map <- county_map %>% rename(long = x, lat = y)
# See which counties are not in both datasets
county_slopes %>% anti_join(county_map, by = "county")
county_map %>% anti_join(county_slopes, by = "county")
# Rename crime_names county
county_slopes  <- county_slopes  %>%
mutate(county = ifelse(county == "Baltimore City", "Baltimore city", county))
# Merge the map and slope data frames
both_data <-
full_join(county_map, county_slopes, by = "county")
# Peek at the data
head(both_data)
# Set the notebook's plot settings
options(repr.plot.width=10, repr.plot.height=5)
# Plot the results
crime_map <-  ggplot(data = both_data,
aes(x = x, y = y,
group = county,
fill = YEAR_3)) +
geom_polygon() +
scale_fill_continuous(name = expression(atop("Change in crime rate",
"(Number year"^-1*")")),
low = "skyblue", high = "gold")
# Look at the map
crime_map
# Plot the results
crime_map <-  ggplot(data = both_data,
aes(x = long, y = lat,
group = county,
fill = YEAR_3)) +
geom_polygon() +
scale_fill_continuous(name = expression(atop("Change in crime rate",
"(Number year"^-1*")")),
low = "skyblue", high = "gold")
# Look at the map
crime_map
# Plot options
options(repr.plot.width=10, repr.plot.height=5)
# Polish figure
crime_map_final <- crime_map +
theme_minimal() +
xlab("") +
ylab("") +
theme(axis.line = element_blank(),
axis.text = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.border = element_blank(),
panel.background = element_blank())
# Look at the map
print(crime_map_final)
# Build a lmer with both year and population
lmer_pop <- lmer(formula = crime_rate ~ YEAR_3 + POPULATION + (YEAR_3|JURISDICTION), data = crime_use)
# Inspect the results
summary(lmer_pop)
ranef(lmer_pop)
